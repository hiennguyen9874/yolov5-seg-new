{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import tensorrt as trt\n",
    "from PIL import Image\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function is generalized for multiple inputs/outputs.\n",
    "# inputs and outputs are expected to be lists of HostDeviceMem objects.\n",
    "def do_inference(context, bindings, inputs, outputs, stream, batch_size=1):\n",
    "    # Transfer input data to the GPU.\n",
    "    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n",
    "    # Run inference.\n",
    "    context.execute_async(\n",
    "        batch_size=batch_size, bindings=bindings, stream_handle=stream.handle\n",
    "    )\n",
    "    # Transfer predictions back from the GPU.\n",
    "    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n",
    "    # Synchronize the stream\n",
    "    stream.synchronize()\n",
    "    # Return only the host outputs.\n",
    "    return [out.host for out in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simple helper data class that's a little nicer to use than a 2-tuple.\n",
    "class HostDeviceMem(object):\n",
    "    def __init__(self, host_mem, device_mem):\n",
    "        self.host = host_mem\n",
    "        self.device = device_mem\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "\n",
    "\n",
    "def allocate_buffers(\n",
    "    engine,\n",
    "    binding_to_type={\"Input\": np.float32, \"NMS\": np.float32, \"NMS_1\": np.int32},\n",
    "    max_batch_size=-1,\n",
    "):\n",
    "    \"\"\"Allocates host and device buffer for TRT engine inference.\n",
    "    This function is similair to the one in common.py, but\n",
    "    converts network outputs (which are np.float32) appropriately\n",
    "    before writing them to Python buffer. This is needed, since\n",
    "    TensorRT plugins doesn't support output type description, and\n",
    "    in our particular case, we use NMS plugin as network output.\n",
    "    Args:\n",
    "        engine (trt.ICudaEngine): TensorRT engine\n",
    "    Returns:\n",
    "        inputs [HostDeviceMem]: engine input memory\n",
    "        outputs [HostDeviceMem]: engine output memory\n",
    "        bindings [int]: buffer to device bindings\n",
    "        stream (cuda.Stream): cuda stream for engine inference synchronization\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    bindings = []\n",
    "    stream = cuda.Stream()\n",
    "\n",
    "    # Current NMS implementation in TRT only supports DataType.FLOAT but\n",
    "    # it may change in the future, which could brake this sample here\n",
    "    # when using lower precision [e.g. NMS output would not be np.float32\n",
    "    # anymore, even though this is assumed in binding_to_type]\n",
    "\n",
    "    for binding in engine:\n",
    "        size = trt.volume(engine.get_binding_shape(binding)) * max_batch_size\n",
    "        \n",
    "        dtype = binding_to_type[str(binding)]\n",
    "        # Allocate host and device buffers\n",
    "        host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "        device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "        # Append the device buffer to device bindings.\n",
    "        bindings.append(int(device_mem))\n",
    "        # Append to the appropriate list.\n",
    "        if engine.binding_is_input(binding):\n",
    "            inputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "        else:\n",
    "            outputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "    return inputs, outputs, bindings, stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRT_LOGGER = trt.Logger(trt.Logger.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_engine(trt_runtime, engine_path):\n",
    "    assert os.path.exists(engine_path)\n",
    "    with open(engine_path, \"rb\") as f:\n",
    "        engine_data = f.read()\n",
    "    return trt_runtime.deserialize_cuda_engine(engine_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TRTInference(object):\n",
    "    \"\"\"Manages TensorRT objects for model inference.\"\"\"\n",
    "\n",
    "    def __init__(self, trt_engine_path, max_batch_size):\n",
    "        \"\"\"Initializes TensorRT objects needed for model inference.\n",
    "        Args:\n",
    "            trt_engine_path (str): path where TensorRT engine should be stored\n",
    "        \"\"\"\n",
    "        self.max_batch_size = max_batch_size\n",
    "\n",
    "        # We first load all custom plugins shipped with TensorRT,\n",
    "        # some of them will be needed during inference\n",
    "        trt.init_libnvinfer_plugins(TRT_LOGGER, \"\")\n",
    "\n",
    "        # Initialize runtime needed for loading TensorRT engine from file\n",
    "        self.trt_runtime = trt.Runtime(TRT_LOGGER)\n",
    "        # TRT engine placeholder\n",
    "        self.trt_engine = None\n",
    "\n",
    "        # Display requested engine settings to stdout\n",
    "        print(\"TensorRT inference engine settings:\")\n",
    "\n",
    "        # If we get here, the file with engine exists, so we can load it\n",
    "        print(\"Loading cached TensorRT engine from {}\".format(trt_engine_path))\n",
    "        self.trt_engine = load_engine(self.trt_runtime, trt_engine_path)\n",
    "\n",
    "        print(self.trt_engine.max_batch_size)\n",
    "\n",
    "        self.binding_to_type = dict()\n",
    "        for index in range(self.trt_engine.num_bindings):\n",
    "            name = self.trt_engine.get_binding_name(index)\n",
    "            dtype = trt.nptype(self.trt_engine.get_binding_dtype(index))\n",
    "            shape = tuple(self.trt_engine.get_binding_shape(index))\n",
    "            shape = list(map(lambda x: 1 if x == -1 else x, shape))\n",
    "            # data = torch.from_numpy(np.empty(shape, dtype=np.dtype(dtype))).to(device)\n",
    "            self.binding_to_type[name] = dtype\n",
    "\n",
    "        # This allocates memory for network inputs/outputs on both CPU and GPU\n",
    "        (self.inputs, self.outputs, self.bindings, self.stream,) = allocate_buffers(\n",
    "            self.trt_engine,\n",
    "            self.binding_to_type,\n",
    "            self.max_batch_size,  # for dynamic shapes\n",
    "        )\n",
    "\n",
    "        # Execution context is needed for inference\n",
    "        self.context = self.trt_engine.create_execution_context()\n",
    "\n",
    "    def __call__(self, img, batch_size, image_size):\n",
    "        # Copy it into appropriate place into memory\n",
    "        # (self.inputs was returned earlier by allocate_buffers())\n",
    "        np.copyto(self.inputs[0].host, img.ravel())\n",
    "\n",
    "        # When infering on single image, we measure inference\n",
    "        # time to output it to the user\n",
    "        inference_start_time = time.time()\n",
    "\n",
    "        if self.max_batch_size == -1:\n",
    "            # Dynamic\n",
    "            self.context.set_binding_shape(0, (batch_size, 3, image_size, image_size))\n",
    "\n",
    "        # Fetch output from the model\n",
    "        outputs = do_inference(\n",
    "            self.context,\n",
    "            bindings=self.bindings,\n",
    "            inputs=self.inputs,\n",
    "            outputs=self.outputs,\n",
    "            stream=self.stream,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        # Output inference time\n",
    "        print(\n",
    "            \"TensorRT inference time: {} ms\".format(\n",
    "                int(round((time.time() - inference_start_time) * 1000))\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # And return results\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT inference engine settings:\n",
      "Loading cached TensorRT engine from ../weights/yolov5n-seg.trt\n",
      "[10/13/2023-16:57:44] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "\n",
      "[10/13/2023-16:57:44] [TRT] [I] Loaded engine size: 9 MiB\n",
      "[10/13/2023-16:57:44] [TRT] [W] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.\n",
      "[10/13/2023-16:57:44] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 1658, GPU 511 (MiB)\n",
      "[10/13/2023-16:57:44] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 1658, GPU 521 (MiB)\n",
      "[10/13/2023-16:57:44] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +10, now: CPU 0, GPU 31 (MiB)\n",
      "1\n",
      "(1, 3, 640, 640)\n",
      "(-1, 3)\n",
      "[10/13/2023-16:57:44] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3885/226422836.py:27: DeprecationWarning: Use network created with NetworkDefinitionCreationFlag::EXPLICIT_BATCH flag instead.\n",
      "  print(self.trt_engine.max_batch_size)\n",
      "/tmp/ipykernel_3885/226422836.py:31: DeprecationWarning: Use get_tensor_name instead.\n",
      "  name = self.trt_engine.get_binding_name(index)\n",
      "/tmp/ipykernel_3885/226422836.py:32: DeprecationWarning: Use get_tensor_dtype instead.\n",
      "  dtype = trt.nptype(self.trt_engine.get_binding_dtype(index))\n",
      "/tmp/ipykernel_3885/226422836.py:33: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  shape = tuple(self.trt_engine.get_binding_shape(index))\n",
      "/tmp/ipykernel_3885/1040227413.py:36: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  size = trt.volume(engine.get_binding_shape(binding)) * max_batch_size\n",
      "/tmp/ipykernel_3885/1040227413.py:38: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  print(engine.get_binding_shape(binding))\n",
      "/tmp/ipykernel_3885/1040227413.py:47: DeprecationWarning: Use get_tensor_mode instead.\n",
      "  if engine.binding_is_input(binding):\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "cuMemHostAlloc failed: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      4\u001b[0m max_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTRTInference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../weights/yolov5n-seg.trt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 39\u001b[0m, in \u001b[0;36mTRTInference.__init__\u001b[0;34m(self, trt_engine_path, max_batch_size)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinding_to_type[name] \u001b[38;5;241m=\u001b[39m dtype\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# This allocates memory for network inputs/outputs on both CPU and GPU\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbindings, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream,) \u001b[38;5;241m=\u001b[39m \u001b[43mallocate_buffers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrt_engine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinding_to_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# for dynamic shapes\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Execution context is needed for inference\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrt_engine\u001b[38;5;241m.\u001b[39mcreate_execution_context()\n",
      "Cell \u001b[0;32mIn[11], line 42\u001b[0m, in \u001b[0;36mallocate_buffers\u001b[0;34m(engine, binding_to_type, max_batch_size)\u001b[0m\n\u001b[1;32m     40\u001b[0m dtype \u001b[38;5;241m=\u001b[39m binding_to_type[\u001b[38;5;28mstr\u001b[39m(binding)]\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Allocate host and device buffers\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m host_mem \u001b[38;5;241m=\u001b[39m \u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpagelocked_empty\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m device_mem \u001b[38;5;241m=\u001b[39m cuda\u001b[38;5;241m.\u001b[39mmem_alloc(host_mem\u001b[38;5;241m.\u001b[39mnbytes)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Append the device buffer to device bindings.\u001b[39;00m\n",
      "\u001b[0;31mMemoryError\u001b[0m: cuMemHostAlloc failed: out of memory"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "batch_size = 1\n",
    "max_batch_size = 1\n",
    "model = TRTInference(\"../weights/yolov5n-seg.trt\", max_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q101zl7-_aHd"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-DABSAOw4Ri"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "img = cv2.imread(\"../data/images/bus.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox(\n",
    "    im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleup=True, stride=32\n",
    "):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = im.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(\n",
    "        im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color\n",
    "    )  # add border\n",
    "    return im, r, (dw, dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(boxes, r, dwdh):\n",
    "    dwdh = torch.tensor(dwdh * 2).to(boxes.device)\n",
    "    boxes -= dwdh\n",
    "    boxes /= r\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    \"person\",\n",
    "    \"bicycle\",\n",
    "    \"car\",\n",
    "    \"motorcycle\",\n",
    "    \"airplane\",\n",
    "    \"bus\",\n",
    "    \"train\",\n",
    "    \"truck\",\n",
    "    \"boat\",\n",
    "    \"traffic light\",\n",
    "    \"fire hydrant\",\n",
    "    \"stop sign\",\n",
    "    \"parking meter\",\n",
    "    \"bench\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"dog\",\n",
    "    \"horse\",\n",
    "    \"sheep\",\n",
    "    \"cow\",\n",
    "    \"elephant\",\n",
    "    \"bear\",\n",
    "    \"zebra\",\n",
    "    \"giraffe\",\n",
    "    \"backpack\",\n",
    "    \"umbrella\",\n",
    "    \"handbag\",\n",
    "    \"tie\",\n",
    "    \"suitcase\",\n",
    "    \"frisbee\",\n",
    "    \"skis\",\n",
    "    \"snowboard\",\n",
    "    \"sports ball\",\n",
    "    \"kite\",\n",
    "    \"baseball bat\",\n",
    "    \"baseball glove\",\n",
    "    \"skateboard\",\n",
    "    \"surfboard\",\n",
    "    \"tennis racket\",\n",
    "    \"bottle\",\n",
    "    \"wine glass\",\n",
    "    \"cup\",\n",
    "    \"fork\",\n",
    "    \"knife\",\n",
    "    \"spoon\",\n",
    "    \"bowl\",\n",
    "    \"banana\",\n",
    "    \"apple\",\n",
    "    \"sandwich\",\n",
    "    \"orange\",\n",
    "    \"broccoli\",\n",
    "    \"carrot\",\n",
    "    \"hot dog\",\n",
    "    \"pizza\",\n",
    "    \"donut\",\n",
    "    \"cake\",\n",
    "    \"chair\",\n",
    "    \"couch\",\n",
    "    \"potted plant\",\n",
    "    \"bed\",\n",
    "    \"dining table\",\n",
    "    \"toilet\",\n",
    "    \"tv\",\n",
    "    \"laptop\",\n",
    "    \"mouse\",\n",
    "    \"remote\",\n",
    "    \"keyboard\",\n",
    "    \"cell phone\",\n",
    "    \"microwave\",\n",
    "    \"oven\",\n",
    "    \"toaster\",\n",
    "    \"sink\",\n",
    "    \"refrigerator\",\n",
    "    \"book\",\n",
    "    \"clock\",\n",
    "    \"vase\",\n",
    "    \"scissors\",\n",
    "    \"teddy bear\",\n",
    "    \"hair drier\",\n",
    "    \"toothbrush\",\n",
    "]\n",
    "colors = {\n",
    "    name: [random.randint(0, 255) for _ in range(3)] for i, name in enumerate(names)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tzGt5tP9nJs_",
    "outputId": "b5e4658f-8b25-4926-bf87-dced1f966fff"
   },
   "outputs": [],
   "source": [
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "image = img.copy()\n",
    "image, ratio, dwdh = letterbox(image, auto=False)\n",
    "image = image.transpose((2, 0, 1))\n",
    "image = np.expand_dims(image, 0)\n",
    "image = np.ascontiguousarray(image)\n",
    "im = image.astype(np.float32)\n",
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im = torch.from_numpy(im).to(device)\n",
    "im /= 255.0\n",
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 546
    },
    "id": "xv8UsDWvn9i4",
    "outputId": "b960358f-8993-4b84-c8c8-d169676a014a"
   },
   "outputs": [],
   "source": [
    "result = model(im, batch_size, 640)\n",
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0].shape, result[1].shape, result[2].shape, result[3].shape, result[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_indices = result[0].reshape(-1, 100, 3)\n",
    "selected_boxes = result[1].reshape(-1, 100, 4)\n",
    "selected_categories = result[2].reshape(-1, 100)\n",
    "selected_scores = result[3].reshape(-1, 100)\n",
    "selected_mask = result[4].reshape(-1, 100, 160, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = result[0].reshape(-1, 1)[0]\n",
    "boxes = result[1].reshape(-1, 100, 4)[0]\n",
    "scores = result[2].reshape(-1, 100, 1)[0]\n",
    "classes = result[3].reshape(-1, 100, 1)[0]\n",
    "masks = result[4].reshape(-1, 100, 160 * 160)[0]\n",
    "\n",
    "print(nums.shape)\n",
    "print(boxes.shape)\n",
    "print(scores.shape)\n",
    "print(classes.shape)\n",
    "print(masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = boxes[: nums[0]]\n",
    "scores = scores[: nums[0]]\n",
    "classes = classes[: nums[0]]\n",
    "masks = masks[: nums[0]].reshape(-1, 160, 160)\n",
    "\n",
    "boxes.shape, scores.shape, classes.shape, masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = torch.tensor(nums)\n",
    "boxes = torch.tensor(boxes)\n",
    "scores = torch.tensor(scores)\n",
    "classes = torch.tensor(classes)\n",
    "masks = torch.tensor(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "masks = F.interpolate(\n",
    "    masks.unsqueeze(dim=0), (640, 640), mode=\"bilinear\", align_corners=False\n",
    ")[0].gt_(0.5)\n",
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boxes = boxes.cpu().numpy()\n",
    "scores = scores.cpu().numpy()\n",
    "classes = classes.cpu().numpy()\n",
    "masks = masks.cpu().numpy().astype(bool).reshape(-1, 640, 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nimg = image[0].copy().transpose(1, 2, 0)\n",
    "nimg = nimg.astype(np.uint8)\n",
    "# nimg = cv2.cvtColor(nimg, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "pnimg = nimg.copy()\n",
    "pnimg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bbox, score, cls, mask in zip(boxes, scores, classes, masks):\n",
    "    if score < 0.25:\n",
    "        continue\n",
    "\n",
    "    color = [np.random.randint(255), np.random.randint(255), np.random.randint(255)]\n",
    "\n",
    "    pnimg[mask] = pnimg[mask] * 0.5 + np.array(color, dtype=np.uint8) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# coco example\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(pnimg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "YOLOv7ONNXandTRT.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
